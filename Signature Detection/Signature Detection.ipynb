{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train(train_path, image_size, classes):\n",
    "    images = []\n",
    "    labels = []\n",
    "    img_names = []\n",
    "    cls = []\n",
    "\n",
    "    print('Going to read')\n",
    "    for fields in classes:   \n",
    "        index = classes.index(fields)\n",
    "        print('Going to read {} files (Index: {})'.format(fields, index))\n",
    "        path = os.path.join(train_path, fields, '*g')\n",
    "        files = glob.glob(path)\n",
    "        for fl in files:\n",
    "            image = cv2.imread(fl)\n",
    "            print(fl)\n",
    "            image = cv2.resize(image, (image_size, image_size),0,0, cv2.INTER_LINEAR)\n",
    "            image = image.astype(np.float32)\n",
    "            image = np.multiply(image, 1.0 / 255.0)\n",
    "            images.append(image)\n",
    "            label = np.zeros(len(classes))\n",
    "            label[index] = 1.0\n",
    "            labels.append(label)\n",
    "            flbase = os.path.basename(fl)\n",
    "            img_names.append(flbase)\n",
    "            cls.append(fields)\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    img_names = np.array(img_names)\n",
    "    cls = np.array(cls)\n",
    "\n",
    "    return images, labels, img_names, cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "\n",
    "  def __init__(self, images, labels, img_names, cls):\n",
    "    self._num_examples = images.shape[0]\n",
    "\n",
    "    self._images = images\n",
    "    self._labels = labels\n",
    "    self._img_names = img_names\n",
    "    self._cls = cls\n",
    "    self._epochs_done = 0\n",
    "    self._index_in_epoch = 0\n",
    "\n",
    "  @property\n",
    "  def images(self):\n",
    "    return self._images\n",
    "\n",
    "  @property\n",
    "  def labels(self):\n",
    "    return self._labels\n",
    "\n",
    "  @property\n",
    "  def img_names(self):\n",
    "    return self._img_names\n",
    "\n",
    "  @property\n",
    "  def cls(self):\n",
    "    return self._cls\n",
    "\n",
    "  @property\n",
    "  def num_examples(self):\n",
    "    return self._num_examples\n",
    "\n",
    "  @property\n",
    "  def epochs_done(self):\n",
    "    return self._epochs_done\n",
    "\n",
    "  def next_batch(self, batch_size):\n",
    "\n",
    "    start = self._index_in_epoch\n",
    "    self._index_in_epoch += batch_size\n",
    "\n",
    "    if self._index_in_epoch > self._num_examples:\n",
    "\n",
    "      self._epochs_done += 1\n",
    "      start = 0\n",
    "      self._index_in_epoch = batch_size\n",
    "      assert batch_size <= self._num_examples\n",
    "    end = self._index_in_epoch\n",
    "\n",
    "    return self._images[start:end], self._labels[start:end], self._img_names[start:end], self._cls[start:end]\n",
    "\n",
    "\n",
    "def read_train_sets(train_path, image_size, classes, validation_size):\n",
    "  class DataSets(object):\n",
    "    pass\n",
    "  data_sets = DataSets()\n",
    "\n",
    "  images, labels, img_names, cls = load_train(train_path, image_size, classes)\n",
    "  images, labels, img_names, cls = shuffle(images, labels, img_names, cls)  \n",
    "\n",
    "  if isinstance(validation_size, float):\n",
    "    validation_size = int(validation_size * images.shape[0])\n",
    "\n",
    "  validation_images = images[:validation_size]\n",
    "  validation_labels = labels[:validation_size]\n",
    "  validation_img_names = img_names[:validation_size]\n",
    "  validation_cls = cls[:validation_size]\n",
    "\n",
    "  train_images = images[validation_size:]\n",
    "  train_labels = labels[validation_size:]\n",
    "  train_img_names = img_names[validation_size:]\n",
    "  train_cls = cls[validation_size:]\n",
    "\n",
    "  data_sets.train = DataSet(train_images, train_labels, train_img_names, train_cls)\n",
    "  data_sets.valid = DataSet(validation_images, validation_labels, validation_img_names, validation_cls)\n",
    "\n",
    "  return data_sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to read\n",
      "Going to read c1 files (Index: 0)\n",
      "train_data/c1/001_2.png\n",
      "train_data/c1/001_22.png\n",
      "train_data/c1/001_19.png\n",
      "train_data/c1/001_9.png\n",
      "train_data/c1/001_17.png\n",
      "train_data/c1/001_5.png\n",
      "train_data/c1/001_15.png\n",
      "train_data/c1/001_11.png\n",
      "train_data/c1/001_14.png\n",
      "train_data/c1/001_18.png\n",
      "train_data/c1/001_3.png\n",
      "train_data/c1/001_4.png\n",
      "train_data/c1/001_1.png\n",
      "train_data/c1/001_13.png\n",
      "train_data/c1/001_7.png\n",
      "train_data/c1/001_12.png\n",
      "train_data/c1/001_6.png\n",
      "train_data/c1/001_20.png\n",
      "train_data/c1/001_10.png\n",
      "train_data/c1/001_8.png\n",
      "train_data/c1/001_21.png\n",
      "train_data/c1/001_16.png\n",
      "Going to read c2 files (Index: 1)\n",
      "train_data/c2/002_5.png\n",
      "train_data/c2/002_16.png\n",
      "train_data/c2/002_21.png\n",
      "train_data/c2/002_11.png\n",
      "train_data/c2/002_22.png\n",
      "train_data/c2/002_4.png\n",
      "train_data/c2/002_8.png\n",
      "train_data/c2/002_1.png\n",
      "train_data/c2/002_6.png\n",
      "train_data/c2/002_7.png\n",
      "train_data/c2/002_10.png\n",
      "train_data/c2/002_18.png\n",
      "train_data/c2/002_20.png\n",
      "train_data/c2/002_12.png\n",
      "train_data/c2/002_13.png\n",
      "train_data/c2/002_14.png\n",
      "train_data/c2/002_9.png\n",
      "train_data/c2/002_2.png\n",
      "train_data/c2/002_19.png\n",
      "train_data/c2/002_3.png\n",
      "train_data/c2/002_15.png\n",
      "train_data/c2/002_17.png\n",
      "Going to read c3 files (Index: 2)\n",
      "train_data/c3/003_12.png\n",
      "train_data/c3/003_11.png\n",
      "train_data/c3/003_20.png\n",
      "train_data/c3/003_17.png\n",
      "train_data/c3/003_15.png\n",
      "train_data/c3/003_18.png\n",
      "train_data/c3/003_8.png\n",
      "train_data/c3/003_5.png\n",
      "train_data/c3/003_22.png\n",
      "train_data/c3/003_6.png\n",
      "train_data/c3/003_19.png\n",
      "train_data/c3/003_2.png\n",
      "train_data/c3/003_21.png\n",
      "train_data/c3/003_7.png\n",
      "train_data/c3/003_3.png\n",
      "train_data/c3/003_1.png\n",
      "train_data/c3/003_10.png\n",
      "train_data/c3/003_9.png\n",
      "train_data/c3/003_13.png\n",
      "train_data/c3/003_16.png\n",
      "train_data/c3/003_4.png\n",
      "train_data/c3/003_14.png\n",
      "Going to read c4 files (Index: 3)\n",
      "train_data/c4/004_2.png\n",
      "train_data/c4/004_1.png\n",
      "train_data/c4/004_3.png\n",
      "train_data/c4/004_20.png\n",
      "train_data/c4/004_10.png\n",
      "train_data/c4/004_24.png\n",
      "train_data/c4/004_4.png\n",
      "train_data/c4/004_6.png\n",
      "train_data/c4/004_11.png\n",
      "train_data/c4/004_16.png\n",
      "train_data/c4/004_14.png\n",
      "train_data/c4/004_21.png\n",
      "train_data/c4/004_9.png\n",
      "train_data/c4/004_5.png\n",
      "train_data/c4/004_8.png\n",
      "train_data/c4/004_18.png\n",
      "train_data/c4/004_12.png\n",
      "train_data/c4/004_13.png\n",
      "train_data/c4/004_7.png\n",
      "train_data/c4/004_19.png\n",
      "train_data/c4/004_17.png\n",
      "train_data/c4/004_15.png\n",
      "Going to read c5 files (Index: 4)\n",
      "train_data/c5/005_18.png\n",
      "train_data/c5/005_19.png\n",
      "train_data/c5/005_11.png\n",
      "train_data/c5/005_7.png\n",
      "train_data/c5/005_9.png\n",
      "train_data/c5/005_2.png\n",
      "train_data/c5/005_3.png\n",
      "train_data/c5/005_21.png\n",
      "train_data/c5/005_12.png\n",
      "train_data/c5/005_6.png\n",
      "train_data/c5/005_16.png\n",
      "train_data/c5/005_1.png\n",
      "train_data/c5/005_10.png\n",
      "train_data/c5/005_8.png\n",
      "train_data/c5/005_20.png\n",
      "train_data/c5/005_4.png\n",
      "train_data/c5/005_17.png\n",
      "train_data/c5/005_15.png\n",
      "train_data/c5/005_5.png\n",
      "Going to read c6 files (Index: 5)\n",
      "train_data/c6/006_8.png\n",
      "train_data/c6/006_2.png\n",
      "train_data/c6/006_17.png\n",
      "train_data/c6/006_18.png\n",
      "train_data/c6/006_4.png\n",
      "train_data/c6/006_16.png\n",
      "train_data/c6/006_1.png\n",
      "train_data/c6/006_13.png\n",
      "train_data/c6/006_3.png\n",
      "train_data/c6/006_7.png\n",
      "train_data/c6/006_6.png\n",
      "train_data/c6/006_21.png\n",
      "train_data/c6/006_12.png\n",
      "train_data/c6/006_5.png\n",
      "train_data/c6/006_9.png\n",
      "train_data/c6/006_14.png\n",
      "train_data/c6/006_10.png\n",
      "train_data/c6/006_11.png\n",
      "train_data/c6/006_20.png\n",
      "train_data/c6/006_15.png\n",
      "Going to read c7 files (Index: 6)\n",
      "train_data/c7/007_11.png\n",
      "train_data/c7/007_4.png\n",
      "train_data/c7/007_9.png\n",
      "train_data/c7/007_5.png\n",
      "train_data/c7/007_22.png\n",
      "train_data/c7/007_21.png\n",
      "train_data/c7/007_13.png\n",
      "train_data/c7/007_6.png\n",
      "train_data/c7/007_16.png\n",
      "train_data/c7/007_20.png\n",
      "train_data/c7/007_1.png\n",
      "train_data/c7/007_17.png\n",
      "train_data/c7/007_18.png\n",
      "train_data/c7/007_14.png\n",
      "train_data/c7/007_7.png\n",
      "train_data/c7/007_15.png\n",
      "train_data/c7/007_19.png\n",
      "train_data/c7/007_3.png\n",
      "train_data/c7/007_12.png\n",
      "train_data/c7/007_2.png\n",
      "train_data/c7/007_10.png\n",
      "train_data/c7/007_8.png\n",
      "Going to read c8 files (Index: 7)\n",
      "train_data/c8/008_8.png\n",
      "train_data/c8/008_20.png\n",
      "train_data/c8/008_12.png\n",
      "train_data/c8/008_22.png\n",
      "train_data/c8/008_4.png\n",
      "train_data/c8/008_11.png\n",
      "train_data/c8/008_17.png\n",
      "train_data/c8/008_9.png\n",
      "train_data/c8/008_18.png\n",
      "train_data/c8/008_13.png\n",
      "train_data/c8/008_19.png\n",
      "train_data/c8/008_1.png\n",
      "train_data/c8/008_5.png\n",
      "train_data/c8/008_14.png\n",
      "train_data/c8/008_2.png\n",
      "train_data/c8/008_6.png\n",
      "train_data/c8/008_10.png\n",
      "train_data/c8/008_16.png\n",
      "train_data/c8/008_15.png\n",
      "train_data/c8/008_3.png\n",
      "train_data/c8/008_21.png\n",
      "train_data/c8/008_7.png\n",
      "Going to read c9 files (Index: 8)\n",
      "train_data/c9/009_6.png\n",
      "train_data/c9/009_13.png\n",
      "train_data/c9/009_9.png\n",
      "train_data/c9/009_22.png\n",
      "train_data/c9/009_15.png\n",
      "train_data/c9/009_8.png\n",
      "train_data/c9/009_5.png\n",
      "train_data/c9/009_17.png\n",
      "train_data/c9/009_10.png\n",
      "train_data/c9/009_11.png\n",
      "train_data/c9/009_21.png\n",
      "train_data/c9/009_4.png\n",
      "train_data/c9/009_1.png\n",
      "train_data/c9/009_14.png\n",
      "train_data/c9/009_20.png\n",
      "train_data/c9/009_7.png\n",
      "train_data/c9/009_12.png\n",
      "train_data/c9/009_2.png\n",
      "train_data/c9/009_3.png\n",
      "train_data/c9/009_16.png\n",
      "train_data/c9/009_19.png\n",
      "train_data/c9/009_18.png\n",
      "Going to read c10 files (Index: 9)\n",
      "train_data/c10/010_17.png\n",
      "train_data/c10/010_8.png\n",
      "train_data/c10/010_7.png\n",
      "train_data/c10/010_13.png\n",
      "train_data/c10/010_2.png\n",
      "train_data/c10/010_20.png\n",
      "train_data/c10/010_1.png\n",
      "train_data/c10/010_16.png\n",
      "train_data/c10/010_22.png\n",
      "train_data/c10/010_6.png\n",
      "train_data/c10/010_19.png\n",
      "train_data/c10/010_11.png\n",
      "train_data/c10/010_15.png\n",
      "train_data/c10/010_10.png\n",
      "train_data/c10/010_12.png\n",
      "train_data/c10/010_14.png\n",
      "train_data/c10/010_3.png\n",
      "train_data/c10/010_21.png\n",
      "train_data/c10/010_9.png\n",
      "train_data/c10/010_5.png\n",
      "train_data/c10/010_4.png\n",
      "train_data/c10/010_18.png\n",
      "Going to read d1 files (Index: 10)\n",
      "Going to read d2 files (Index: 11)\n",
      "Going to read d3 files (Index: 12)\n",
      "Going to read d4 files (Index: 13)\n",
      "Going to read d6 files (Index: 14)\n",
      "Going to read d9 files (Index: 15)\n",
      "Going to read d12 files (Index: 16)\n",
      "Going to read d14 files (Index: 17)\n",
      "Going to read d15 files (Index: 18)\n",
      "Going to read d16 files (Index: 19)\n",
      "Going to read o1 files (Index: 20)\n",
      "train_data/o1/1_4.png\n",
      "train_data/o1/1_2.png\n",
      "train_data/o1/1_17.png\n",
      "train_data/o1/1_22.png\n",
      "train_data/o1/1_5.png\n",
      "train_data/o1/1_10.png\n",
      "train_data/o1/1_3.png\n",
      "train_data/o1/1_6.png\n",
      "train_data/o1/1_7.png\n",
      "train_data/o1/1_13.png\n",
      "train_data/o1/1_14.png\n",
      "train_data/o1/1_18.png\n",
      "train_data/o1/1_19.png\n",
      "train_data/o1/1_15.png\n",
      "train_data/o1/1_16.png\n",
      "train_data/o1/1_20.png\n",
      "train_data/o1/1_11.png\n",
      "train_data/o1/1_8.png\n",
      "train_data/o1/1_12.png\n",
      "train_data/o1/1_21.png\n",
      "train_data/o1/1_9.png\n",
      "train_data/o1/1_1.png\n",
      "Going to read o2 files (Index: 21)\n",
      "train_data/o2/2_20.png\n",
      "train_data/o2/2_22.png\n",
      "train_data/o2/2_3.png\n",
      "train_data/o2/2_6.png\n",
      "train_data/o2/2_5.png\n",
      "train_data/o2/2_14.png\n",
      "train_data/o2/2_16.png\n",
      "train_data/o2/2_17.png\n",
      "train_data/o2/2_18.png\n",
      "train_data/o2/2_7.png\n",
      "train_data/o2/2_9.png\n",
      "train_data/o2/2_2.png\n",
      "train_data/o2/2_19.png\n",
      "train_data/o2/2_11.png\n",
      "train_data/o2/2_4.png\n",
      "train_data/o2/2_12.png\n",
      "train_data/o2/2_15.png\n",
      "train_data/o2/2_10.png\n",
      "train_data/o2/2_21.png\n",
      "train_data/o2/2_8.png\n",
      "train_data/o2/2_13.png\n",
      "train_data/o2/2_1.png\n",
      "Going to read o3 files (Index: 22)\n",
      "train_data/o3/3_15.png\n",
      "train_data/o3/3_1.png\n",
      "train_data/o3/3_13.png\n",
      "train_data/o3/3_2.png\n",
      "train_data/o3/3_14.png\n",
      "train_data/o3/3_7.png\n",
      "train_data/o3/3_20.png\n",
      "train_data/o3/3_5.png\n",
      "train_data/o3/3_4.png\n",
      "train_data/o3/3_19.png\n",
      "train_data/o3/3_21.png\n",
      "train_data/o3/3_16.png\n",
      "train_data/o3/3_22.png\n",
      "train_data/o3/3_3.png\n",
      "train_data/o3/3_17.png\n",
      "train_data/o3/3_18.png\n",
      "train_data/o3/3_8.png\n",
      "train_data/o3/3_9.png\n",
      "train_data/o3/3_10.png\n",
      "train_data/o3/3_6.png\n",
      "train_data/o3/3_11.png\n",
      "train_data/o3/3_12.png\n",
      "Going to read o4 files (Index: 23)\n",
      "train_data/o4/4_14.png\n",
      "train_data/o4/4_19.png\n",
      "train_data/o4/4_5.png\n",
      "train_data/o4/4_8.png\n",
      "train_data/o4/4_6.png\n",
      "train_data/o4/4_18.png\n",
      "train_data/o4/4_11.png\n",
      "train_data/o4/4_2.png\n",
      "train_data/o4/4_10.png\n",
      "train_data/o4/4_20.png\n",
      "train_data/o4/4_17.png\n",
      "train_data/o4/4_12.png\n",
      "train_data/o4/4_9.png\n",
      "train_data/o4/4_7.png\n",
      "train_data/o4/4_1.png\n",
      "train_data/o4/4_3.png\n",
      "train_data/o4/4_13.png\n",
      "train_data/o4/4_22.png\n",
      "train_data/o4/4_15.png\n",
      "train_data/o4/4_21.png\n",
      "train_data/o4/4_16.png\n",
      "train_data/o4/4_4.png\n",
      "Going to read o5 files (Index: 24)\n",
      "train_data/o5/5_4.png\n",
      "train_data/o5/5_12.png\n",
      "train_data/o5/5_14.png\n",
      "train_data/o5/5_16.png\n",
      "train_data/o5/5_8.png\n",
      "train_data/o5/5_19.png\n",
      "train_data/o5/5_17.png\n",
      "train_data/o5/5_5.png\n",
      "train_data/o5/5_7.png\n",
      "train_data/o5/5_6.png\n",
      "train_data/o5/5_22.png\n",
      "train_data/o5/5_15.png\n",
      "train_data/o5/5_13.png\n",
      "train_data/o5/5_10.png\n",
      "train_data/o5/5_1.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data/o5/5_2.png\n",
      "train_data/o5/5_21.png\n",
      "train_data/o5/5_20.png\n",
      "train_data/o5/5_11.png\n",
      "train_data/o5/5_3.png\n",
      "train_data/o5/5_18.png\n",
      "train_data/o5/5_9.png\n",
      "Going to read o6 files (Index: 25)\n",
      "train_data/o6/6_1.png\n",
      "train_data/o6/6_6.png\n",
      "train_data/o6/6_12.png\n",
      "train_data/o6/6_8.png\n",
      "train_data/o6/6_13.png\n",
      "train_data/o6/6_20.png\n",
      "train_data/o6/6_11.png\n",
      "train_data/o6/6_4.png\n",
      "train_data/o6/6_14.png\n",
      "train_data/o6/6_15.png\n",
      "train_data/o6/6_5.png\n",
      "train_data/o6/6_22.png\n",
      "train_data/o6/6_9.png\n",
      "train_data/o6/6_17.png\n",
      "train_data/o6/6_2.png\n",
      "train_data/o6/6_16.png\n",
      "train_data/o6/6_21.png\n",
      "train_data/o6/6_3.png\n",
      "train_data/o6/6_7.png\n",
      "train_data/o6/6_10.png\n",
      "train_data/o6/6_18.png\n",
      "train_data/o6/6_19.png\n",
      "Going to read o7 files (Index: 26)\n",
      "train_data/o7/7_12.png\n",
      "train_data/o7/7_3.png\n",
      "train_data/o7/7_7.png\n",
      "train_data/o7/7_19.png\n",
      "train_data/o7/7_4.png\n",
      "train_data/o7/7_11.png\n",
      "train_data/o7/7_16.png\n",
      "train_data/o7/7_22.png\n",
      "train_data/o7/7_17.png\n",
      "train_data/o7/7_5.png\n",
      "train_data/o7/7_13.png\n",
      "train_data/o7/7_10.png\n",
      "train_data/o7/7_9.png\n",
      "train_data/o7/7_8.png\n",
      "train_data/o7/7_2.png\n",
      "train_data/o7/7_6.png\n",
      "train_data/o7/7_21.png\n",
      "train_data/o7/7_18.png\n",
      "train_data/o7/7_15.png\n",
      "train_data/o7/7_20.png\n",
      "train_data/o7/7_14.png\n",
      "train_data/o7/7_1.png\n",
      "Going to read o8 files (Index: 27)\n",
      "train_data/o8/8_19.png\n",
      "train_data/o8/8_10.png\n",
      "train_data/o8/8_6.png\n",
      "train_data/o8/8_8.png\n",
      "train_data/o8/8_22.png\n",
      "train_data/o8/8_7.png\n",
      "train_data/o8/8_3.png\n",
      "train_data/o8/8_11.png\n",
      "train_data/o8/8_4.png\n",
      "train_data/o8/8_14.png\n",
      "train_data/o8/8_21.png\n",
      "train_data/o8/8_16.png\n",
      "train_data/o8/8_1.png\n",
      "train_data/o8/8_5.png\n",
      "train_data/o8/8_20.png\n",
      "train_data/o8/8_9.png\n",
      "train_data/o8/8_15.png\n",
      "train_data/o8/8_13.png\n",
      "train_data/o8/8_18.png\n",
      "train_data/o8/8_2.png\n",
      "train_data/o8/8_17.png\n",
      "train_data/o8/8_12.png\n",
      "Going to read o10 files (Index: 28)\n",
      "train_data/o10/10_15.png\n",
      "train_data/o10/10_11.png\n",
      "train_data/o10/10_16.png\n",
      "train_data/o10/10_10.png\n",
      "train_data/o10/10_1.png\n",
      "train_data/o10/10_19.png\n",
      "train_data/o10/10_8.png\n",
      "train_data/o10/10_2.png\n",
      "train_data/o10/10_7.png\n",
      "train_data/o10/10_22.png\n",
      "train_data/o10/10_21.png\n",
      "train_data/o10/10_20.png\n",
      "train_data/o10/10_17.png\n",
      "train_data/o10/10_3.png\n",
      "train_data/o10/10_14.png\n",
      "train_data/o10/10_13.png\n",
      "train_data/o10/10_6.png\n",
      "train_data/o10/10_9.png\n",
      "train_data/o10/10_5.png\n",
      "train_data/o10/10_18.png\n",
      "train_data/o10/10_12.png\n",
      "train_data/o10/10_4.png\n",
      "Going to read o11 files (Index: 29)\n",
      "train_data/o11/11_22.png\n",
      "train_data/o11/11_13.png\n",
      "train_data/o11/11_11.png\n",
      "train_data/o11/11_12.png\n",
      "train_data/o11/11_9.png\n",
      "train_data/o11/11_8.png\n",
      "train_data/o11/11_1.png\n",
      "train_data/o11/11_6.png\n",
      "train_data/o11/11_3.png\n",
      "train_data/o11/11_15.png\n",
      "train_data/o11/11_4.png\n",
      "train_data/o11/11_16.png\n",
      "train_data/o11/11_21.png\n",
      "train_data/o11/11_18.png\n",
      "train_data/o11/11_10.png\n",
      "train_data/o11/11_7.png\n",
      "train_data/o11/11_14.png\n",
      "train_data/o11/11_19.png\n",
      "train_data/o11/11_20.png\n",
      "train_data/o11/11_2.png\n",
      "train_data/o11/11_17.png\n",
      "train_data/o11/11_5.png\n",
      "Going to read o12 files (Index: 30)\n",
      "train_data/o12/12_5.png\n",
      "train_data/o12/12_16.png\n",
      "train_data/o12/12_10.png\n",
      "train_data/o12/12_14.png\n",
      "train_data/o12/12_4.png\n",
      "train_data/o12/12_21.png\n",
      "train_data/o12/12_7.png\n",
      "train_data/o12/12_17.png\n",
      "train_data/o12/12_11.png\n",
      "train_data/o12/12_2.png\n",
      "train_data/o12/12_22.png\n",
      "train_data/o12/12_9.png\n",
      "train_data/o12/12_8.png\n",
      "train_data/o12/12_3.png\n",
      "train_data/o12/12_6.png\n",
      "train_data/o12/12_1.png\n",
      "train_data/o12/12_12.png\n",
      "train_data/o12/12_15.png\n",
      "train_data/o12/12_13.png\n",
      "train_data/o12/12_18.png\n",
      "train_data/o12/12_20.png\n",
      "train_data/o12/12_19.png\n",
      "Going to read o13 files (Index: 31)\n",
      "train_data/o13/13_20.png\n",
      "train_data/o13/13_22.png\n",
      "train_data/o13/13_9.png\n",
      "train_data/o13/13_10.png\n",
      "train_data/o13/13_1.png\n",
      "train_data/o13/13_4.png\n",
      "train_data/o13/13_5.png\n",
      "train_data/o13/13_15.png\n",
      "train_data/o13/13_12.png\n",
      "train_data/o13/13_17.png\n",
      "train_data/o13/13_13.png\n",
      "train_data/o13/13_6.png\n",
      "train_data/o13/13_18.png\n",
      "train_data/o13/13_7.png\n",
      "train_data/o13/13_14.png\n",
      "train_data/o13/13_3.png\n",
      "train_data/o13/13_2.png\n",
      "train_data/o13/13_11.png\n",
      "train_data/o13/13_21.png\n",
      "train_data/o13/13_19.png\n",
      "train_data/o13/13_16.png\n",
      "train_data/o13/13_8.png\n",
      "Going to read o14 files (Index: 32)\n",
      "train_data/o14/14_2.png\n",
      "train_data/o14/14_14.png\n",
      "train_data/o14/14_17.png\n",
      "train_data/o14/14_5.png\n",
      "train_data/o14/14_8.png\n",
      "train_data/o14/14_11.png\n",
      "train_data/o14/14_6.png\n",
      "train_data/o14/14_9.png\n",
      "train_data/o14/14_12.png\n",
      "train_data/o14/14_13.png\n",
      "train_data/o14/14_4.png\n",
      "train_data/o14/14_3.png\n",
      "train_data/o14/14_21.png\n",
      "train_data/o14/14_18.png\n",
      "train_data/o14/14_7.png\n",
      "train_data/o14/14_19.png\n",
      "train_data/o14/14_20.png\n",
      "train_data/o14/14_22.png\n",
      "train_data/o14/14_10.png\n",
      "train_data/o14/14_1.png\n",
      "train_data/o14/14_16.png\n",
      "train_data/o14/14_15.png\n",
      "Going to read o15 files (Index: 33)\n",
      "train_data/o15/15_14.png\n",
      "train_data/o15/15_12.png\n",
      "train_data/o15/15_10.png\n",
      "train_data/o15/15_6.png\n",
      "train_data/o15/15_18.png\n",
      "train_data/o15/15_11.png\n",
      "train_data/o15/15_7.png\n",
      "train_data/o15/15_15.png\n",
      "train_data/o15/15_19.png\n",
      "train_data/o15/15_1.png\n",
      "train_data/o15/15_4.png\n",
      "train_data/o15/15_9.png\n",
      "train_data/o15/15_3.png\n",
      "train_data/o15/15_13.png\n",
      "train_data/o15/15_2.png\n",
      "train_data/o15/15_8.png\n",
      "train_data/o15/15_21.png\n",
      "train_data/o15/15_5.png\n",
      "train_data/o15/15_22.png\n",
      "train_data/o15/15_20.png\n",
      "train_data/o15/15_17.png\n",
      "train_data/o15/15_16.png\n",
      "Going to read o16 files (Index: 34)\n",
      "train_data/o16/16_10.png\n",
      "train_data/o16/16_18.png\n",
      "train_data/o16/16_3.png\n",
      "train_data/o16/16_13.png\n",
      "train_data/o16/16_4.png\n",
      "train_data/o16/16_2.png\n",
      "train_data/o16/16_21.png\n",
      "train_data/o16/16_14.png\n",
      "train_data/o16/16_12.png\n",
      "train_data/o16/16_20.png\n",
      "train_data/o16/16_7.png\n",
      "train_data/o16/16_11.png\n",
      "train_data/o16/16_22.png\n",
      "train_data/o16/16_16.png\n",
      "train_data/o16/16_6.png\n",
      "train_data/o16/16_5.png\n",
      "train_data/o16/16_9.png\n",
      "train_data/o16/16_17.png\n",
      "train_data/o16/16_19.png\n",
      "train_data/o16/16_8.png\n",
      "train_data/o16/16_15.png\n",
      "train_data/o16/16_1.png\n",
      "Going to read o17 files (Index: 35)\n",
      "train_data/o17/17_17.png\n",
      "train_data/o17/17_16.png\n",
      "train_data/o17/17_11.png\n",
      "train_data/o17/17_2.png\n",
      "train_data/o17/17_12.png\n",
      "train_data/o17/17_15.png\n",
      "train_data/o17/17_5.png\n",
      "train_data/o17/17_4.png\n",
      "train_data/o17/17_18.png\n",
      "train_data/o17/17_20.png\n",
      "train_data/o17/17_6.png\n",
      "train_data/o17/17_1.png\n",
      "train_data/o17/17_9.png\n",
      "train_data/o17/17_3.png\n",
      "train_data/o17/17_19.png\n",
      "train_data/o17/17_7.png\n",
      "train_data/o17/17_21.png\n",
      "train_data/o17/17_14.png\n",
      "train_data/o17/17_22.png\n",
      "train_data/o17/17_13.png\n",
      "train_data/o17/17_10.png\n",
      "train_data/o17/17_8.png\n",
      "Going to read o18 files (Index: 36)\n",
      "train_data/o18/18_21.png\n",
      "train_data/o18/18_9.png\n",
      "train_data/o18/18_8.png\n",
      "train_data/o18/18_17.png\n",
      "train_data/o18/18_4.png\n",
      "train_data/o18/18_18.png\n",
      "train_data/o18/18_10.png\n",
      "train_data/o18/18_12.png\n",
      "train_data/o18/18_7.png\n",
      "train_data/o18/18_19.png\n",
      "train_data/o18/18_22.png\n",
      "train_data/o18/18_1.png\n",
      "train_data/o18/18_15.png\n",
      "train_data/o18/18_16.png\n",
      "train_data/o18/18_14.png\n",
      "train_data/o18/18_20.png\n",
      "train_data/o18/18_3.png\n",
      "train_data/o18/18_2.png\n",
      "train_data/o18/18_11.png\n",
      "train_data/o18/18_5.png\n",
      "train_data/o18/18_6.png\n",
      "train_data/o18/18_13.png\n",
      "Going to read o19 files (Index: 37)\n",
      "train_data/o19/19_12.png\n",
      "train_data/o19/19_22.png\n",
      "train_data/o19/19_4.png\n",
      "train_data/o19/19_3.png\n",
      "train_data/o19/19_13.png\n",
      "train_data/o19/19_9.png\n",
      "train_data/o19/19_5.png\n",
      "train_data/o19/19_17.png\n",
      "train_data/o19/19_15.png\n",
      "train_data/o19/19_1.png\n",
      "train_data/o19/19_11.png\n",
      "train_data/o19/19_18.png\n",
      "train_data/o19/19_7.png\n",
      "train_data/o19/19_6.png\n",
      "train_data/o19/19_21.png\n",
      "train_data/o19/19_16.png\n",
      "train_data/o19/19_14.png\n",
      "train_data/o19/19_19.png\n",
      "train_data/o19/19_8.png\n",
      "train_data/o19/19_20.png\n",
      "train_data/o19/19_10.png\n",
      "train_data/o19/19_2.png\n",
      "successfully read\n",
      "Training-set:\t\t489\n",
      "Validation-set:\t122\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "\n",
    "classes = ['c1','c2','c3','c4','c5','c6','c7','c8','c9','c10','d1','d2','d3','d4','d6','d9','d12','d14','d15','d16','o1','o2','o3','o4','o5','o6','o7','o8','o10','o11','o12','o13','o14','o15','o16','o17','o18','o19']\n",
    "class_number = len(classes)\n",
    "\n",
    "train_accuracy=[]\n",
    "validation_accuracy=[]\n",
    "validation_loss=[]\n",
    "epoc_list=[]\n",
    "\n",
    "validation_size = 0.2\n",
    "image_size = 128\n",
    "channel_number = 3\n",
    "train_path='train_data'\n",
    "\n",
    "\n",
    "data = read_train_sets(train_path, image_size, classes, validation_size=validation_size)\n",
    "\n",
    "\n",
    "print(\"successfully read\")\n",
    "print(\"Training-set:\\t\\t{}\".format(len(data.train.labels)))\n",
    "print(\"Validation-set:\\t{}\".format(len(data.valid.labels)))\n",
    "\n",
    "\n",
    "\n",
    "session = tf.Session()\n",
    "x = tf.placeholder(tf.float32, shape=[None, image_size,image_size,channel_number], name='x')\n",
    "\n",
    "\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, class_number], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, dimension=1)\n",
    "\n",
    "\n",
    "filter_size_con1 = 3\n",
    "filter_number_con1 = 32\n",
    "\n",
    "filter_size_con2 = 3\n",
    "filter_number_con2 = 32\n",
    "\n",
    "filter_size_con3 = 3\n",
    "filter_number_con3 = 64\n",
    "\n",
    "filter_size_con4 = 3\n",
    "filter_number_con4 = 64\n",
    "\n",
    "fc_layer_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "def create_biases(size):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[size]))\n",
    "\n",
    "\n",
    "def create_convolutional_layer(input,\n",
    "               input_channel_number, \n",
    "               conv_filter_size,        \n",
    "               filter_number):  \n",
    "    \n",
    "\n",
    "    weights = create_weights(shape=[conv_filter_size, conv_filter_size, input_channel_number, filter_number])\n",
    "    biases = create_biases(filter_number)\n",
    "\n",
    "\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                     filter=weights,\n",
    "                     strides=[1, 1, 1, 1],\n",
    "                     padding='SAME')\n",
    "\n",
    "    layer += biases\n",
    "\n",
    "\n",
    "    layer = tf.nn.max_pool(value=layer,\n",
    "                            ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1],\n",
    "                            padding='SAME')\n",
    "\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "def create_flatten_layer(layer):\n",
    "    layer_shape = layer.get_shape()\n",
    "    feature_number = layer_shape[1:4].num_elements()\n",
    "    layer = tf.reshape(layer, [-1, feature_number])\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "def create_fc_layer(input,          \n",
    "             input_number,    \n",
    "             output_number,\n",
    "             use_relu=True):\n",
    "    \n",
    "\n",
    "    weights = create_weights(shape=[input_number, output_number])\n",
    "    biases = create_biases(output_number)\n",
    "\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_con1 = create_convolutional_layer(input=x,\n",
    "               input_channel_number=channel_number,\n",
    "               conv_filter_size=filter_size_con1,\n",
    "               filter_number=filter_number_con1)\n",
    "layer_con2 = create_convolutional_layer(input=layer_con1,\n",
    "               input_channel_number=filter_number_con1,\n",
    "               conv_filter_size=filter_size_con2,\n",
    "               filter_number=filter_number_con2)\n",
    "\n",
    "layer_con3= create_convolutional_layer(input=layer_con2,\n",
    "               input_channel_number=filter_number_con2,\n",
    "               conv_filter_size=filter_size_con3,\n",
    "               filter_number=filter_number_con3)\n",
    "\n",
    "layer_con4= create_convolutional_layer(input=layer_con3,\n",
    "               input_channel_number=filter_number_con3,\n",
    "               conv_filter_size=filter_size_con4,\n",
    "               filter_number=filter_number_con4)\n",
    "          \n",
    "flat_layer = create_flatten_layer(layer_con4)\n",
    "\n",
    "layer_fc1 = create_fc_layer(input=flat_layer,\n",
    "                     input_number=flat_layer.get_shape()[1:4].num_elements(),\n",
    "                     output_number=fc_layer_size,\n",
    "                     use_relu=True)\n",
    "\n",
    "layer_fc2 = create_fc_layer(input=layer_fc1,\n",
    "                     input_number=fc_layer_size,\n",
    "                     output_number=fc_layer_size,\n",
    "                     use_relu=True) \n",
    "\n",
    "\t\t\t\t\t \n",
    "\n",
    "layer_fc3 = create_fc_layer(input=layer_fc2,\n",
    "                     input_number=fc_layer_size,\n",
    "                     output_number=class_number,\n",
    "                     use_relu=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.nn.softmax(layer_fc3,name='y_pred')\n",
    "\n",
    "y_pred_cls = tf.argmax(y_pred, dimension=1)\n",
    "session.run(tf.global_variables_initializer())\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc3,\n",
    "                                                    labels=y_true)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "session.run(tf.global_variables_initializer()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello: /home/bishwa/Documents/ML/Signature Detection\n",
      "Training Epoch 1 --- Training Accuracy: 100.0%, Validation Accuracy:  90.6%,  Validation Loss: 0.187\n",
      "Training Epoch 2 --- Training Accuracy: 100.0%, Validation Accuracy:  90.6%,  Validation Loss: 0.186\n",
      "Training Epoch 3 --- Training Accuracy: 100.0%, Validation Accuracy:  90.6%,  Validation Loss: 0.186\n",
      "Training Epoch 4 --- Training Accuracy: 100.0%, Validation Accuracy:  90.6%,  Validation Loss: 0.186\n",
      "Training Epoch 5 --- Training Accuracy: 100.0%, Validation Accuracy:  90.6%,  Validation Loss: 0.185\n",
      "Training Epoch 6 --- Training Accuracy: 100.0%, Validation Accuracy:  90.6%,  Validation Loss: 0.185\n",
      "Training Epoch 7 --- Training Accuracy: 100.0%, Validation Accuracy:  90.6%,  Validation Loss: 0.185\n",
      "Training Epoch 8 --- Training Accuracy: 100.0%, Validation Accuracy:  90.6%,  Validation Loss: 0.184\n",
      "Training Epoch 9 --- Training Accuracy: 100.0%, Validation Accuracy:  90.6%,  Validation Loss: 0.184\n",
      "Training Epoch 10 --- Training Accuracy: 100.0%, Validation Accuracy:  90.6%,  Validation Loss: 0.184\n",
      "Training Epoch 11 --- Training Accuracy: 100.0%, Validation Accuracy:  90.6%,  Validation Loss: 0.186\n",
      "Training Epoch 12 --- Training Accuracy: 100.0%, Validation Accuracy:  90.6%,  Validation Loss: 0.183\n",
      "Training Epoch 13 --- Training Accuracy: 100.0%, Validation Accuracy:  90.6%,  Validation Loss: 0.183\n",
      "Training Epoch 14 --- Training Accuracy: 100.0%, Validation Accuracy:  90.6%,  Validation Loss: 0.182\n"
     ]
    }
   ],
   "source": [
    "def show_progress(epoch, feed_dict_train, feed_dict_validate, val_loss):\n",
    "\t\n",
    "\tacc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "\tval_acc = session.run(accuracy, feed_dict=feed_dict_validate)\n",
    "\tmsg = \"Training Epoch {0} --- Training Accuracy: {1:>6.1%}, Validation Accuracy: {2:>6.1%},  Validation Loss: {3:.3f}\"\n",
    "\tprint (msg.format (epoch + 1, acc, val_acc, val_loss) )\n",
    "\n",
    "total_iterations = 0\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "def train(iteration_number):\n",
    "    global total_iterations\n",
    "    print ('Hello: ' +os.getcwd())\n",
    "    for i in range(total_iterations,\n",
    "                   total_iterations + iteration_number):\n",
    "\n",
    "        x_batch, y_true_batch, _, cls_batch = data.train.next_batch(batch_size)\n",
    "        x_valid_batch, y_valid_batch, _, valid_cls_batch = data.valid.next_batch(batch_size)\n",
    "\n",
    "        \n",
    "        feed_dict_tr = {x: x_batch,\n",
    "                           y_true: y_true_batch}\n",
    "        feed_dict_val = {x: x_valid_batch,\n",
    "                              y_true: y_valid_batch}\n",
    "\n",
    "        session.run(optimizer, feed_dict=feed_dict_tr)\n",
    "\n",
    "        if i % int(data.train.num_examples/batch_size) == 0: \n",
    "            val_loss = session.run(cost, feed_dict=feed_dict_val)\n",
    "            epoch = int(i / int(data.train.num_examples/batch_size))    \n",
    "            \n",
    "            show_progress(epoch, feed_dict_tr, feed_dict_val, val_loss)\n",
    "            saver.save(session, os.getcwd()+'\\\\signature')\n",
    "    total_iterations += iteration_number\n",
    "train(iteration_number=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, glob, cv2\n",
    "import sys, argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir = '/home/bishwa/Documents/ML/Signature%20Detection'\n",
    "image_path = 'test_data'\n",
    "filename = path_dir + '/' + image_path + '/'\n",
    "path = os.path.join('test_data', filename, '*g')\n",
    "files = glob.glob(path)\n",
    "\n",
    "\n",
    "classes = ['c1','c2','c3','c4','c5','c6','c7','c8','c9','c10','d1','d2','d3','d4','d6','d9','d12','d14','d15','d16','o1','o2','o3','o4','o5','o6','o7','o8','o10','o11','o12','o13','o14','o15','o16','o17','o18','o19']\n",
    "class_number = len(classes)\n",
    "image_size = 128\n",
    "channel_number = 3\n",
    "\n",
    "totalExperimented =0;\n",
    "truePositive=0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in files:\n",
    "\n",
    "    filename = i\n",
    "\n",
    "    p = 0\n",
    "    className = \"\"\n",
    "    chk2=0;\n",
    "    i = i[::-1]\n",
    "    for kk in i:\n",
    "        if kk == '.':\n",
    "            chk2=1;\n",
    "        elif kk == '\\\\':\n",
    "            break;\n",
    "        elif kk == '/':\n",
    "            break;\n",
    "        elif chk2 == 1:\n",
    "            className = className + kk;\n",
    "\n",
    "\n",
    "    className = className[::-1]\n",
    "\n",
    "    images = []\n",
    "    image = cv2.imread(filename)\n",
    "    image = cv2.resize(image, (image_size, image_size), 0, 0, cv2.INTER_LINEAR)\n",
    "    images.append(image)\n",
    "    images = np.array(images, dtype=np.uint8)\n",
    "    images = images.astype('float32')\n",
    "    images = np.multiply(images, 1.0 / 255.0)\n",
    "    x_batch = images.reshape(1, image_size, image_size, channel_number)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.import_meta_graph('signature.meta')\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    graph = tf.get_default_graph()\n",
    "\n",
    "    y_pred = graph.get_tensor_by_name(\"y_pred:0\")\n",
    "    x = graph.get_tensor_by_name(\"x:0\")\n",
    "    y_true = graph.get_tensor_by_name(\"y_true:0\")\n",
    "    y_test_images = np.zeros((1, class_number))\n",
    "\n",
    "    feed_dict_testing = {x: x_batch, y_true: y_test_images}\n",
    "    result = sess.run(y_pred, feed_dict=feed_dict_testing)\n",
    " \n",
    "    j = 1;\n",
    "    val =0;\n",
    "    probableClass = 0;\n",
    "    for ii in result:\n",
    "        chk=0\n",
    "        for jj in ii:\n",
    "            chk=chk+1\n",
    "            if jj>val:\n",
    "                val =jj\n",
    "                probableClass = chk\n",
    "    totalExperimented = totalExperimented +1\n",
    "    if val >= 0.90:\n",
    "\n",
    "        print('Photo Name       : ', className)\n",
    "        print('predicted person : ', classes[probableClass - 1])\n",
    "        print('Probability value: ', val)\n",
    "        truePositive = truePositive + 1\n",
    "    else:\n",
    "        print('Photo Name       : ', className)\n",
    "        print('predicted person : Unknown')\n",
    "        print('Probability value: ', val)\n",
    "\n",
    "    print()\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
